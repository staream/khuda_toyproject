{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19ecdc94-0f2c-43ec-8885-2323725fdb8e",
   "metadata": {},
   "source": [
    "참고 영상\n",
    "- Age and Gender Detection Using Classification and Regression | Deep Learning Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ed24a2c-53fd-4a28-b2bb-b56b1d826aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import load_img\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Conv2D, Dropout, Flatten, MaxPooling2D, Input\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from tqdm.notebook import tqdm\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b0998362-e01e-4b29-96d7-604248f2a14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Base_dir = './image_dataset'\n",
    "image_classnames = os.listdir(Base_dir)\n",
    "image_classnames\n",
    "image_paths =[]\n",
    "clothes_labels =[]\n",
    "clothes_input=[]\n",
    "target_image_paths=[]\n",
    "\n",
    "for image_class in image_classnames:\n",
    "    image_filenames = os.listdir(f'{Base_dir}/{image_class}')\n",
    "    for image in image_filenames:\n",
    "        if image == f\"{image_class}_re.png\":\n",
    "            clothes_label = image\n",
    "            #clothes_labels.append(clothes_label)\n",
    "            clothes_labels.append(clothes_label)      # target\n",
    "            clothes_input.append(image)               # input\n",
    "            image_paths.append(f'{Base_dir}/{image_class}/{image}')\n",
    "            target_image_paths.append(int(image_class))\n",
    "        else:\n",
    "            clothes_labels.append(clothes_label)      # target\n",
    "            clothes_input.append(image)               # input\n",
    "            image_paths.append(f'{Base_dir}/{image_class}/{image}')\n",
    "            target_image_paths.append(int(image_class))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8201e5d1-5e11-4c68-902a-1bbf4be8a3cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "171 171 171\n"
     ]
    }
   ],
   "source": [
    "print(len(image_paths),len(clothes_input),len(clothes_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4b966b16-b16d-48d7-9783-4a14a364d0b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_path</th>\n",
       "      <th>input_image</th>\n",
       "      <th>target_image_path</th>\n",
       "      <th>target_image</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>./image_dataset/0/0_re.png</td>\n",
       "      <td>0_re.png</td>\n",
       "      <td>0</td>\n",
       "      <td>0_re.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>./image_dataset/0/0_train_1.png</td>\n",
       "      <td>0_train_1.png</td>\n",
       "      <td>0</td>\n",
       "      <td>0_re.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>./image_dataset/0/0_train_10.png</td>\n",
       "      <td>0_train_10.png</td>\n",
       "      <td>0</td>\n",
       "      <td>0_re.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>./image_dataset/0/0_train_11.png</td>\n",
       "      <td>0_train_11.png</td>\n",
       "      <td>0</td>\n",
       "      <td>0_re.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>./image_dataset/0/0_train_12.png</td>\n",
       "      <td>0_train_12.png</td>\n",
       "      <td>0</td>\n",
       "      <td>0_re.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>./image_dataset/9/9_train_2.png</td>\n",
       "      <td>9_train_2.png</td>\n",
       "      <td>9</td>\n",
       "      <td>9_re.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>./image_dataset/9/9_train_3.png</td>\n",
       "      <td>9_train_3.png</td>\n",
       "      <td>9</td>\n",
       "      <td>9_re.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>./image_dataset/9/9_train_4.png</td>\n",
       "      <td>9_train_4.png</td>\n",
       "      <td>9</td>\n",
       "      <td>9_re.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>./image_dataset/9/9_train_5.png</td>\n",
       "      <td>9_train_5.png</td>\n",
       "      <td>9</td>\n",
       "      <td>9_re.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>./image_dataset/9/9_train_6.png</td>\n",
       "      <td>9_train_6.png</td>\n",
       "      <td>9</td>\n",
       "      <td>9_re.png</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>171 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           image_path     input_image  target_image_path  \\\n",
       "0          ./image_dataset/0/0_re.png        0_re.png                  0   \n",
       "1     ./image_dataset/0/0_train_1.png   0_train_1.png                  0   \n",
       "2    ./image_dataset/0/0_train_10.png  0_train_10.png                  0   \n",
       "3    ./image_dataset/0/0_train_11.png  0_train_11.png                  0   \n",
       "4    ./image_dataset/0/0_train_12.png  0_train_12.png                  0   \n",
       "..                                ...             ...                ...   \n",
       "166   ./image_dataset/9/9_train_2.png   9_train_2.png                  9   \n",
       "167   ./image_dataset/9/9_train_3.png   9_train_3.png                  9   \n",
       "168   ./image_dataset/9/9_train_4.png   9_train_4.png                  9   \n",
       "169   ./image_dataset/9/9_train_5.png   9_train_5.png                  9   \n",
       "170   ./image_dataset/9/9_train_6.png   9_train_6.png                  9   \n",
       "\n",
       "    target_image  \n",
       "0       0_re.png  \n",
       "1       0_re.png  \n",
       "2       0_re.png  \n",
       "3       0_re.png  \n",
       "4       0_re.png  \n",
       "..           ...  \n",
       "166     9_re.png  \n",
       "167     9_re.png  \n",
       "168     9_re.png  \n",
       "169     9_re.png  \n",
       "170     9_re.png  \n",
       "\n",
       "[171 rows x 4 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame()\n",
    "df['image_path'],df['input_image'],df['target_image_path'],df['target_image'] = image_paths,clothes_input,target_image_paths,clothes_labels\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd162cb6-70fc-4d37-8a4d-afd0f5e08606",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3af69d1f-381d-454f-97ae-2487aef5a454",
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL \n",
    "from PIL import Image\n",
    "\n",
    "PIL.Image.ANTIALIAS =PIL.Image.LANCZOS\n",
    "rand_index = random.randint(0, len(image_paths))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ca0205-9b97-4241-a4e3-3d0c6cf6e18c",
   "metadata": {},
   "source": [
    "# Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "beb11704-8713-4aaa-9a98-5e7d1fdb8757",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_image_features(images):\n",
    "    features = list()\n",
    "\n",
    "    for image in tqdm(images):\n",
    "        img = load_img(image)#, grayscale=True) # loading image as numpy array\n",
    "        img = img.resize((128,128), Image.ANTIALIAS) # ANTIALIAS : help in cases where there are Distortion\n",
    "        img = np.array(img)\n",
    "        features.append(img)\n",
    "    features = np.array(features)\n",
    "    features = features.reshape(len(features), 128,128,3) # considering RGB\n",
    "    return features\n",
    "\n",
    "def y_image_features(images):\n",
    "    features = list()\n",
    "    for image in tqdm(images):\n",
    "        img = load_img(image)#, grayscale=True) # loading image as numpy array\n",
    "        img = img.resize((128,128), Image.ANTIALIAS) # ANTIALIAS : help in cases where there are Distortion\n",
    "        img = np.array(img)\n",
    "        features.append(img)\n",
    "    features = np.array(features)\n",
    "    features = features.reshape(len(features), 128,128,3) # considering RGB\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e6e28b58-fd00-4ebd-87a7-174718836b06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5844557453e046f5adde25aec013a1f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/171 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X = extract_image_features(df['image_path'])\n",
    "Y = np.array(df['target_image_path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "bae93979-4b9e-4248-be28-c9583c5e4d8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(171, 128, 128, 3)\n",
      "(171,)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e4d8eb34-c6d4-45f9-8fd4-81ae08b9073a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nomalizing\n",
    "X = X/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e5560ca8-e4dc-49b6-bea1-55633b98ee0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (128,128,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7774ac16-2985-4b49-9cfe-ed24fc32febb",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = Input((input_shape))\n",
    "conv_1 =Conv2D(32,kernel_size=(3,3),activation='relu')(inputs)\n",
    "max_1 = MaxPooling2D(pool_size=(2,2))(conv_1)\n",
    "conv_2 =Conv2D(64,kernel_size=(3,3),activation='relu')(max_1)\n",
    "max_2 = MaxPooling2D(pool_size=(2,2))(conv_2)\n",
    "conv_3 =Conv2D(128,kernel_size=(3,3),activation='relu')(max_2)\n",
    "max_3 = MaxPooling2D(pool_size=(2,2))(conv_3)\n",
    "conv_4 =Conv2D(256,kernel_size=(3,3),activation='relu')(max_3)\n",
    "max_4 = MaxPooling2D(pool_size=(2,1))(conv_4)\n",
    "\n",
    "flatten = Flatten()(max_4)\n",
    "\n",
    "#fully connected layers\n",
    "dense_1 = Dense(256, activation = 'relu')(flatten)\n",
    "\n",
    "dropout_1 = Dropout(0.3)(dense_1)\n",
    "\n",
    "output_1 = Dense(1, activation = 'relu' , name = 'img_out')(dropout_1)\n",
    "# not binary >> x sigmoid, binary_crossentropy / estimate >> relu + mae\n",
    "\n",
    "model = Model(inputs=[inputs], outputs=[output_1])\n",
    "\n",
    "model.compile(loss=['mae'], optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "52309ec8-3dcc-44be-b75e-02bf904b974c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 634ms/step - accuracy: 0.0578 - loss: 4.6480 - val_accuracy: 0.0000e+00 - val_loss: 4.3394\n",
      "Epoch 2/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 582ms/step - accuracy: 0.0322 - loss: 3.2706 - val_accuracy: 0.0000e+00 - val_loss: 5.4592\n",
      "Epoch 3/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 637ms/step - accuracy: 0.0322 - loss: 3.4070 - val_accuracy: 0.0000e+00 - val_loss: 5.3323\n",
      "Epoch 4/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 512ms/step - accuracy: 0.0413 - loss: 3.1425 - val_accuracy: 0.0000e+00 - val_loss: 4.6530\n",
      "Epoch 5/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 531ms/step - accuracy: 0.0379 - loss: 3.0156 - val_accuracy: 0.0000e+00 - val_loss: 5.4917\n",
      "Epoch 6/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 535ms/step - accuracy: 0.0457 - loss: 3.5014 - val_accuracy: 0.0000e+00 - val_loss: 5.0472\n",
      "Epoch 7/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 684ms/step - accuracy: 0.0509 - loss: 3.2600 - val_accuracy: 0.0000e+00 - val_loss: 4.2856\n",
      "Epoch 8/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 547ms/step - accuracy: 0.0240 - loss: 3.0989 - val_accuracy: 0.0000e+00 - val_loss: 4.2187\n",
      "Epoch 9/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 630ms/step - accuracy: 0.0348 - loss: 3.0599 - val_accuracy: 0.0000e+00 - val_loss: 4.4974\n",
      "Epoch 10/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 640ms/step - accuracy: 0.0386 - loss: 2.9232 - val_accuracy: 0.0000e+00 - val_loss: 4.1620\n",
      "Epoch 11/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 494ms/step - accuracy: 0.0292 - loss: 3.1127 - val_accuracy: 0.0000e+00 - val_loss: 3.9153\n",
      "Epoch 12/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 437ms/step - accuracy: 0.0407 - loss: 2.7007 - val_accuracy: 0.0000e+00 - val_loss: 4.3645\n",
      "Epoch 13/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 421ms/step - accuracy: 0.0754 - loss: 2.6558 - val_accuracy: 0.0000e+00 - val_loss: 4.0999\n",
      "Epoch 14/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 543ms/step - accuracy: 0.0814 - loss: 2.5572 - val_accuracy: 0.0000e+00 - val_loss: 4.8268\n",
      "Epoch 15/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 628ms/step - accuracy: 0.0543 - loss: 2.5829 - val_accuracy: 0.0000e+00 - val_loss: 4.2239\n",
      "Epoch 16/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 569ms/step - accuracy: 0.0718 - loss: 2.2942 - val_accuracy: 0.0000e+00 - val_loss: 4.3524\n",
      "Epoch 17/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 529ms/step - accuracy: 0.0546 - loss: 2.0167 - val_accuracy: 0.0000e+00 - val_loss: 3.3109\n",
      "Epoch 18/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 528ms/step - accuracy: 0.0574 - loss: 2.0947 - val_accuracy: 0.0000e+00 - val_loss: 4.2644\n",
      "Epoch 19/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 502ms/step - accuracy: 0.0961 - loss: 1.8842 - val_accuracy: 0.0000e+00 - val_loss: 4.4027\n",
      "Epoch 20/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 706ms/step - accuracy: 0.0760 - loss: 2.3660 - val_accuracy: 0.0000e+00 - val_loss: 3.8559\n",
      "Epoch 21/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 629ms/step - accuracy: 0.0832 - loss: 2.1304 - val_accuracy: 0.0000e+00 - val_loss: 4.1550\n",
      "Epoch 22/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 507ms/step - accuracy: 0.1150 - loss: 1.9281 - val_accuracy: 0.0000e+00 - val_loss: 4.5386\n",
      "Epoch 23/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 611ms/step - accuracy: 0.1040 - loss: 2.1696 - val_accuracy: 0.0000e+00 - val_loss: 3.7032\n",
      "Epoch 24/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 607ms/step - accuracy: 0.1065 - loss: 1.8216 - val_accuracy: 0.0000e+00 - val_loss: 3.7987\n",
      "Epoch 25/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 448ms/step - accuracy: 0.1100 - loss: 1.9268 - val_accuracy: 0.0000e+00 - val_loss: 4.1171\n",
      "Epoch 26/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 637ms/step - accuracy: 0.1227 - loss: 1.8038 - val_accuracy: 0.0000e+00 - val_loss: 3.6091\n",
      "Epoch 27/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 529ms/step - accuracy: 0.1337 - loss: 1.7263 - val_accuracy: 0.0000e+00 - val_loss: 4.3104\n",
      "Epoch 28/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 577ms/step - accuracy: 0.1066 - loss: 1.6291 - val_accuracy: 0.0000e+00 - val_loss: 3.4184\n",
      "Epoch 29/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 621ms/step - accuracy: 0.1040 - loss: 1.6880 - val_accuracy: 0.0000e+00 - val_loss: 3.4675\n",
      "Epoch 30/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 479ms/step - accuracy: 0.1353 - loss: 1.7748 - val_accuracy: 0.0000e+00 - val_loss: 2.7888\n",
      "Epoch 31/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 562ms/step - accuracy: 0.1001 - loss: 2.0571 - val_accuracy: 0.0000e+00 - val_loss: 4.2138\n",
      "Epoch 32/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 479ms/step - accuracy: 0.1038 - loss: 1.8714 - val_accuracy: 0.0000e+00 - val_loss: 3.7373\n",
      "Epoch 33/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 584ms/step - accuracy: 0.1134 - loss: 1.5137 - val_accuracy: 0.0000e+00 - val_loss: 3.3026\n",
      "Epoch 34/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 460ms/step - accuracy: 0.1173 - loss: 1.8997 - val_accuracy: 0.0000e+00 - val_loss: 4.1920\n",
      "Epoch 35/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 591ms/step - accuracy: 0.1367 - loss: 1.5133 - val_accuracy: 0.0000e+00 - val_loss: 3.8571\n",
      "Epoch 36/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 557ms/step - accuracy: 0.1192 - loss: 1.6021 - val_accuracy: 0.0000e+00 - val_loss: 4.2083\n",
      "Epoch 37/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 598ms/step - accuracy: 0.0864 - loss: 1.7246 - val_accuracy: 0.0000e+00 - val_loss: 3.8909\n",
      "Epoch 38/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 609ms/step - accuracy: 0.1195 - loss: 1.7581 - val_accuracy: 0.0000e+00 - val_loss: 3.8845\n",
      "Epoch 39/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 491ms/step - accuracy: 0.1428 - loss: 1.4676 - val_accuracy: 0.0000e+00 - val_loss: 3.5663\n",
      "Epoch 40/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 494ms/step - accuracy: 0.1337 - loss: 1.5654 - val_accuracy: 0.0000e+00 - val_loss: 3.2310\n",
      "Epoch 41/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 217ms/step - accuracy: 0.1432 - loss: 1.8405 - val_accuracy: 0.0000e+00 - val_loss: 3.8438\n",
      "Epoch 42/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 410ms/step - accuracy: 0.1050 - loss: 1.4337 - val_accuracy: 0.0000e+00 - val_loss: 3.6082\n",
      "Epoch 43/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 553ms/step - accuracy: 0.1549 - loss: 1.4384 - val_accuracy: 0.0000e+00 - val_loss: 4.0400\n",
      "Epoch 44/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 637ms/step - accuracy: 0.1133 - loss: 1.5321 - val_accuracy: 0.0000e+00 - val_loss: 3.5014\n",
      "Epoch 45/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 690ms/step - accuracy: 0.1078 - loss: 1.5931 - val_accuracy: 0.0000e+00 - val_loss: 3.1726\n",
      "Epoch 46/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 533ms/step - accuracy: 0.1437 - loss: 1.2463 - val_accuracy: 0.0000e+00 - val_loss: 3.1638\n",
      "Epoch 47/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 542ms/step - accuracy: 0.1411 - loss: 1.4269 - val_accuracy: 0.0000e+00 - val_loss: 3.9585\n",
      "Epoch 48/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 566ms/step - accuracy: 0.1113 - loss: 1.4841 - val_accuracy: 0.0000e+00 - val_loss: 3.3605\n",
      "Epoch 49/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 537ms/step - accuracy: 0.1176 - loss: 1.3533 - val_accuracy: 0.0000e+00 - val_loss: 3.1447\n",
      "Epoch 50/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 478ms/step - accuracy: 0.1201 - loss: 1.4759 - val_accuracy: 0.0000e+00 - val_loss: 4.2034\n"
     ]
    }
   ],
   "source": [
    "image_classifier = model.fit(x=X, y=Y, batch_size=32, epochs=50, validation_split=0.2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
